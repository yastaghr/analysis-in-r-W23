---
title: "MARINE MICROPLASTICS DATASET RELATIONSHIP ANALYSIS IN R"
author: "Yassy Bardallis, Doppler Bardallis"
date: "2023-03-10"
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("oce")
install.packages("interp")
library(interp)
library(oce)
library(Matrix)
SEA_MICRO <- read.csv("SEA_MICRO.csv")
colnames(SEA_MICRO) <- c("times", "latitude", "longitude", "pieces_km2")
```

## DATASET

### ORIGIN AND PERTINENCE OF THIS DATASET

For this project, we have chosen to work with a dataset on marine microplastic density in samples drawn from seawater worldwide. These datasets contain information quantifying the presence of microplastics at specific points in the world on specific dates. This dataset is termed the "SEA_MICRO" dataset and was acquired from [kaggle.com](https://www.kaggle.com/datasets/brsdincer/marine-microplastic-on-world-density-noaa) on 2023-02-24 at 10:43:05; however, the original data was used in the project funded by a grant from the NOAA Marine Debris Program to the University of Washington, Tacoma [referenced here](https://marinedebris.noaa.gov/technical-memorandum/laboratory-methods-analysis-microplastics-marine-environment) which sought to create a series of simple standardized methods for collecting and measuring the quantity of microplastics in water samples, samples of sediments from the bed of the body of water, and personal care products.

Marine microplastics, defined as plastic particles under 5mm, pervade our ecosystem. They have been found in everything from [fish](https://www.mdpi.com/2673-8929/1/1/12) to [human placentae](https://ehp.niehs.nih.gov/doi/10.1289/EHP10873); while we are still exploring the impact they have on our world, they are known to be detrimental to the health and life of coral, one study of this phenomenon [can be found here](https://www.nature.com/articles/s42003-021-01961-1). Other species, including us, have been and are being investigated. [You can learn more about this ongoing research here](http://libproxy.uoregon.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=147699782&login.asp&site=ehost-live&scope=site). As the marine world is intricately intertwined with our own, research into their movements is imperative.

### MAKEUP OF THE DATASET

The 7755 observations within this dataset are each described by the date they were collected, the longitude and latitude at which they were collected, and the number of pieces of microplastic per kilometer squared. This last value was determined by the method found on pages 10-19 of the manual that was written and published according to the project's goal [found here](https://repository.library.noaa.gov/view/noaa/10296). These samples are not necessarily ordered or random but likely were drawn from sources available to the researchers at the time. The authors of this project could not determine this because neither the manual nor the poster of the Kaggle dataset recorded this information. Due to this lack of knowledge, we cannot determine the amount of noise in this dataset. We could not discover by whom the original water samples were collected or by what groups. However, we believe that, due to the standardized method used to process this data referred to above, any error in the content of this dataset is limited only by the limitations of our knowledge rather than any disparity in collection methodology.\
We did not preprocess this data in any way; however, it may or may not have undergone this process before its posting on Kaggle. We know of at least three projects that have used this dataset in the past, although there may be more. The manual itself [can be found here](https://repository.library.noaa.gov/view/noaa/10296). Two more projects were connected to the dataset posting on Kaggle; a project in marine data visualization [which can be found here](https://www.kaggle.com/code/virajkadam/marine-plastic-visualization) and a project on the process of analyzing marine data [that can be found here](https://www.kaggle.com/code/brsdincer/marine-micro-plastic-analysis-process). It is undoubtedly a dataset that can be used in future research and analysis of the marine microplastic density and dispersion from 1986 to 2012, albeit with caution due to the uncertain background of that data.

### PROCESSING THE DATASET

A sample of the observations contained within the dataset shows that there exist within it multiple sets of observations taken on the same day at different locations across the globe.

```{r}
knitr::kable(SEA_MICRO[1:16,], caption = "Entries 1-16 of the SEA_MICRO Dataset")
```

This lead us to a problem, as analysis using the `ts()` data format is intended to be done with univariate data which is equally spaced in time. However, it was a useful format for discovering enough basic statistics about this dataset to determine a better format for our analysis. These facts can be seen below.

```{r}
ts.dates.sea_micro <- ts(data = SEA_MICRO$times)
uniq.dates.seamicro <- unique(ts.dates.sea_micro)
#debug point! print("Number of distinct dates on which samples were taken:")
distinct.dates.seamicro <- length(uniq.dates.seamicro)
distinct.dates.seamicro
#debug point! print("Maximum gap between data points:")
spacing.dates.seamicro <- diff(as.Date(SEA_MICRO$times))
gap.max.dates.seamicro <- max(spacing.dates.seamicro)
gap.max.dates.seamicro
#debug point! print("Number of data points with a shared date:")
dup.date.seamicro <- SEA_MICRO$times[duplicated(SEA_MICRO$times)]
length(dup.date.seamicro)
#debug point! print("Number of dates with multiple data points:")
uniq.dup.date.seamicro <- unique(dup.date.seamicro)
length(uniq.dup.date.seamicro)
```

#### OCE PACKAGE

After some digging, we were able to locate a package, `oce`, for processing this data which contains a format that, rather than being dependent on the observation's time, is arranged by the latitude and longitude of the observation. This package is used in the oceanographic community for working with application-specific data in R [@oceanogr2018]. Within this package are several classes to store different types of data; we have chosen to use the `gps-class` as our storage type. In the `oce` package, data is stored in these three slots: `data`, `metadata`, and `processingLog`. For `gps` objects, `data` stores lists of the `Latitude` and `Longitude`, `metadata` stores lists of other types of data (in our case we have `Times` and `Pieces_KM2`), and `processingLog` stores a changelog-like list of text entries.

```{r}
gps.seamicro <- as.gps(SEA_MICRO)
gps.seamicro <- oceSetMetadata(gps.seamicro, name = "pieces_km2", value = SEA_MICRO$pieces_km2, note = "")
gps.seamicro <- oceSetMetadata(gps.seamicro, name = "times", value = as.POSIXct(SEA_MICRO$times), note = "")
```

#### PLOT OF ALL OBSERVATIONS

Before beginning to work with this data set, we found it useful to have a visual of the locations of every data point in this set.

```{r fig.height=5.5, fig.width=6}
data("topoWorld")
mapPlot(longitude = gps.seamicro@data[["longitude"]], latitude = gps.seamicro@data[["latitude"]], grid = FALSE, geographical = 0, col = NULL, clip = FALSE, type = "p", axes = FALSE, projection = "+proj=merc", drawBox = FALSE)
mapImage(topoWorld)
mapGrid(dlongitude = 15, dlatitude = 15, col = "black", lty = "dotdash", lwd = 0.2)
mapPoints(longitude = gps.seamicro@data[["longitude"]], latitude = gps.seamicro@data[["latitude"]], pch = 10, col = "#00b300", cex = 0.75)
title(main = "All Observations in SEA_MICRO", sub = "Observations from 1986 to 2012", outer = FALSE)
mapScalebar(x = "topleft", y = NULL, lwd = 1, col = "black")
```

#### SUBSETTING CHOICE: WEST ATLANTIC AND CARIBEAN

Because the number of total observations in the Pacific ocean is much lower than in the West Atlantic and Caribbean, we have chosen to further limit the dataset we will be working with to the more observed region.

##### LATITUDE

To find this region, we determined the latitudinal quartiles of the whole dataset, then plotted the coordinates of all of the observations with colors to indicate each section. As the points in the set we wish to observe were not above the longitudinal first quartile, we estimated that, if the distance between each quartile were to be used to add another subsection of the data points, then all of the observations we wished to use would be included. We estimated this distance to be 8 degrees of latitude. To include this additional section, we used a minimum latitude of 10 degrees north. This set can be seen below, highlighted in yellow, and is where our subset began.

```{r}
longlat.seamicro <- data.frame("longitude" = SEA_MICRO$longitude, "latitude" = SEA_MICRO$latitude)
plot(longlat.seamicro)
summary(longlat.seamicro)
Q1.longlat <- subset(longlat.seamicro, longlat.seamicro$latitude>=18)
mean.longlat <- subset(longlat.seamicro, longlat.seamicro$latitude>=25)
Q3.longlat <- subset(longlat.seamicro, longlat.seamicro$latitude>=33)
set.longlat <- subset(longlat.seamicro, longlat.seamicro$latitude>=10)
points(set.longlat, col = "yellow")
points(Q1.longlat, col = "red")
points(mean.longlat, col = "green")
points(Q3.longlat, col = "blue")
legend(x = "topleft", legend = c("Q3", "MEAN", "Q1", "CHOSEN"), fill = c("blue", "green", "red", "yellow"), angle = 45, density = 50, cex = 0.45)
```

##### LONGITUDE

Using the set of observations that were determined above, we repeated a similar process with the longitudinal values. We used the median, in this case, because the mean was outside of the needed range. In this case, we eventually arrived at a boundary point at -95 degrees, also known as 95 degrees west.

```{r}
summary(set.longlat)
Q1.set.longlat <- subset(set.longlat, set.longlat$longitude>=-82)
median.set.longlat <- subset(set.longlat, set.longlat$longitude>=-71)
Q3.set.longlat <- subset(set.longlat, set.longlat$longitude>=-65)
subset.longlat <- subset(set.longlat, set.longlat$longitude>=-90)
plot(set.longlat)
points(subset.longlat, col="yellow")
points(Q1.set.longlat, col = "green")
points(median.set.longlat, col = "blue")
points(Q3.set.longlat, col = "red")
```

##### FINAL SUBSET

Our final working set is graphed below.

```{r}
workingset.seamicro <- subset(SEA_MICRO, SEA_MICRO$latitude>=10)
workingset.seamicro <- subset(workingset.seamicro, workingset.seamicro$longitude>=-90)
gps.workingset <- as.gps(workingset.seamicro)
gps.workingset <- oceSetMetadata(gps.workingset, name = "pieces_km2", value = workingset.seamicro$pieces_km2, note = "")
gps.workingset <- oceSetMetadata(gps.workingset, name = "times", value = as.POSIXct(workingset.seamicro$times), note = "")
mapPlot(longitude = gps.workingset@data[["longitude"]], latitude = gps.workingset@data[["latitude"]], grid = FALSE, geographical = 0, col = NULL, clip = FALSE, type = "p", axes = FALSE, projection = "+proj=merc", drawBox = FALSE)
mapImage(topoWorld)
mapGrid(dlongitude = 5, dlatitude = 5, col = "black", lty = "dotdash", lwd = 0.2)
mapPoints(longitude = gps.workingset@data[["longitude"]], latitude = gps.workingset@data[["latitude"]], pch = 10, col = "#500050", cex = 0.2)
mapScalebar(x = "topleft", y = NULL, lwd = 1, col = "black")
title(main = "OBSERVATIONS IN WORKING SET", sub = "Observations from 1986 to 2008", xlab = "LONGITUDE (marks every five degrees)", ylab = "LATITUDE (marks every five degrees)")
```

#### MODEL FOCUS

Due to the nature of this data and the format it is now stored in, we have chosen to train a model to predict the density of microplastics in this area per year, both on average and as a coordinate basis. This means we be making a series of density maps showing the relative amounts of microplastic in this area.

#### VISUAL METHODOLOGY: BARNES INTERPOLATION

To start, though, we created a map showing the density of each area over the entire course of the observation period. The values for all contour maps in this report have been determined with the `oce` package's function `interpBarnes` with one iteration. The `interpBarnes` function uses a Barnes interpolation scheme to estimate the values at all locations on a grid given a sparse and irregular spread of data points. The Barnes interpolation scheme was developed for use with meteorological data[@barnes1964], but is now used in oceanography as well.

##### ROUGH DESCRIPTION OF BARNES INTERPOLATION

This function works in iterations; we have arbitrarily chosen to use one. It makes use of a weighted average of all of the points within a given radius (in our case, we are using the function default) to determine the value of the point on the grid in question.

```{r}
interp.barnes.ws <- interpBarnes(x = workingset.seamicro$longitude, y = workingset.seamicro$latitude, z = workingset.seamicro$pieces_km2, iterations = 1, pregrid = TRUE)
cm.seamicro <- colormap(zlim = range(interp.barnes.ws$zd))
drawPalette(colormap = cm.seamicro)
data("coastlineWorld")
mapPlot(longitude = workingset.seamicro$longitude, latitude = workingset.seamicro$latitude, projection = "+proj=merc", grid = FALSE)
mapImage(latitude = interp.barnes.ws$yg, longitude = interp.barnes.ws$xg, z = interp.barnes.ws$zg, colormap = cm.seamicro)
mapContour(longitude = interp.barnes.ws$xg, latitude = interp.barnes.ws$yg, z = interp.barnes.ws$zg, nlevels = 10, drawlabels = FALSE, col = "red")
mapGrid(dlongitude = 5, dlatitude = 5)
mapLines(coastlineWorld, col = "white")
title(main = "DENSITY MAP AROUND WORKING SET", sub = "Barnes Interpolation with one iteration")
```

### TEST AND TRAINING SETS

#### MODEL LIMITATIONS

Our intended model does not have separate response and predictor variables; rather, we are hoping to use the data as it changes through time to estimate how that data will look in the future. The regression models we studied in class are not well suited to this because of that temporal component [@james2021]; another limiting factor is the inconsistency in the coordinates of data points from year to year that results from the physical constraints that come from working with the ocean and the limited number and track of scientific voyages each year that collected this data.

#### SEPARATION CHOICE

Because of the circumstantial limitations imposed by this dataset, we will be analyzing a model which combines the microplastic density data from past years to estimate the interpolated density of a future year. This model is analogous to a linear regression model; however, rather than a training set and a test set, we have the set of past values and the future values given by the data set.

```{r}
labels.years.ws <- seq(from = 1986, to = 2008, by = 1)
range.years.ws <- data.frame(seq(from = as.POSIXct("1986-01-01", tz = ""), to = as.POSIXct("2008-01-01", tz = ""), by = "year"), seq(from = as.POSIXct("1986-12-31", tz = ""), to = as.POSIXct("2008-12-31", tz = ""), by = "year"))
colnames(range.years.ws) <- c("Start", "End")
rownames(range.years.ws) <- labels.years.ws
ws.1986 <- subset(workingset.seamicro, subset = workingset.seamicro$times<=range.years.ws$End[1])
add.tenths2front <- function(front, v) {
    front <- abs(front);
    add.end <- v[1] - 0.1;
    add.buff <- -0.1 * (front - 1);
    add.start <- add.end + add.buff;
    f.v <- seq(from = add.start, to = add.end, by = 0.1);
    #debug point! return(f.v)
    v2 <- c(f.v, v);
    return(v2)
}
add.tenths2back <- function(back, v) {
    back <- abs(back);
    add.start <- v[length(v)] + 0.1;
    add.buff <- 0.1 * (back - 1);
    add.end <- add.start + add.buff;
    b.v <- seq(from = add.start, to = add.end, by = 0.1);
    v2 <- c(v, b.v);
    return(v2)
}
add.enough.tenths <- function(more, v) {
    if (more%%2 != 0) {
        v <- c(v[1]-0.1, v);
        more <- more - 1;
    }
    #now is not odd
    add.half <- more/2;
    v <- add.tenths2back(add.half, v);
    v <- add.tenths2front(add.half, v);
    #debug point! return(length(v))
    return(v)
}
get.set.gridvals <- function(this.set) {
    this.r.long <- range(this.set$longitude);
    this.r.lat <- range(this.set$latitude);
    this.r.long[1] <- floor(this.r.long[1]);
    this.r.lat[1] <- floor(this.r.lat[1]);
    this.r.long[2] <- ceiling(this.r.long[2]);
    this.r.lat[2] <- ceiling(this.r.lat[2]);
    #now all coords should be included
    xg = seq(from = this.r.long[1], to = this.r.long[2], by = 0.1);
    yg = seq(from = this.r.lat[1], to = this.r.lat[2], by = 0.1);
    #everything is by tenths now
    mod.xg <- length(xg)%%length(this.set$longitude);
    mod.yg <- length(yg)%%length(this.set$latitude);
    if (mod.xg != 0) {
        #the grid does not divide into the coords and it doesn't like this
        needed <- length(this.set$longitude) - mod.xg;
        xg <- add.enough.tenths(needed, xg);
    }
    if (length(yg)%%length(this.set$latitude) != 0) {
        #the grid does not divide into the coords and it doesn't like this
        needed <- length(this.set$latitude) - mod.yg;
        yg <- add.enough.tenths(needed, yg);
    }
    return(list("xg" = xg, "yg" = yg))
}
ws.1986.gv <- get.set.gridvals(ws.1986)
ws.1986.bi <- interpBarnes(x = ws.1986$longitude, y = ws.1986$latitude, z = ws.1986$pieces_km2, xg = ws.1986.gv$xg, yg =  ws.1986.gv$yg, iterations = 1)
```

#### MODEL OUTLINE

We will determine the error of each model by applying the combinatorial weight function to the years prior to make our estimate, then subtraction that from the values given by the observations taken in the target year; we will map this error, determined year by year, against models created by using other weight functions. The goal of this process is to estimate the effect that known past locations of particular amounts of microplastics have on future quantities of microplastics at other locations.

#### IRREDUCIBLE ERROR, VARIANCE, AND BIAS

We know that the irreducible error in this process will be high due to the aforementioned inconsistency and sparsity of our data, the introduction of an unknown amount of microplastics to the ocean in unknown locations, and the unincorporated affect that ocean currents and meteorological events have on the dispersal of said microplastics. Predicting the affects of these influences would be a potential avenue of future research.

## MODEL ANALYSIS

### Model Setup

We will be testing three different models: the first model, in which the data points from previous years are equally weighted; the second model, where each successive year has twice the weight of the year before it, and the third model, where the last three years have twice the power of the one before it, and the rest are equally weighted at half of the weight of the last of those three.

```{r}
model1.weights <- rep(1, times = 23)
model2.weights <- rep(1, times = 23)
x = 22
while (x>0) {
    model2.weights[x] <- 2 * model2.weights[x+1];
    x = x - 1
}
model3.weights <- rep(1, times = 23)
model3.weights[1] <- 8
model3.weights[2] <- 4
model3.weights[3] <- 2
gen.wv.1y <- function(w, year.index) {
    end <- range.years.ws$End[year.index];
    s <- range.years.ws$Start[year.index];
    bef.end <- workingset.seamicro$times[workingset.seamicro$times <= end];
    year.times <- bef.end[bef.end >= s];
    years <- length(year.times);
    #have list of times this year
    if (years<1) {
        return(FALSE)
        #no times that year
    }
    wv <- rep(w, years);
    return(wv)
}
gen.full.wv <- function(model.wv, num.years) {
    full.wv <- NULL;
    #debug point! #debug point! print(num.years);
    num = 1;
    #this starts AT THE OLDEST YEAR INVOLVED because thats what bi needs
    while (num <= num.years) {
        #debug point! #debug point! print(num);
        this.w <- model.wv[num];
        #debug point! #debug point! print(this.w);
        this.wv <- gen.wv.1y(this.w, num);
        #debug point! #debug point! print(this.wv);
        if (this.wv[1] != FALSE) {
            #debug point! #debug point! print(length(this.wv));
            full.wv <- c(full.wv, this.wv);
        }
        num = num + 1;
    }
    #debug point! #debug point! print("out of loop");
    #debug point! #debug point! print(length(full.wv));
    return(full.wv)
}
get.prev.set <- function(num.years) {
    this.set <- subset(workingset.seamicro, workingset.seamicro$times<=range.years.ws$End[num.years]);
    return(this.set)
}
get.this.set <- function(ind.y) {
    half.set <- subset(workingset.seamicro, workingset.seamicro$times<=range.years.ws$End[ind.y]);
    this.set <- subset(half.set, half.set$times>=range.years.ws$Start[ind.y]);
    return(this.set)
}
set.bi.dimnames <- function(bi) {
    bi1 <- bi;
    rownames(bi1$zg) <- bi1$xg;
    colnames(bi1$zg) <- bi1$yg;
    return(bi1)
}
only.in.v <- function(x1, x2) {
    x = NULL;
    for (n in x2) {
        for (m in x1) {
            if (m==n) {
                x <- c(x, m);
            }
        }
    }
    if (is.null(x)) return(FALSE)
    return(x)
}
is.equal.vector <- function(v1, v2) {
    l <- length(v1);
    n <- 1;
    while (n <= l) {
        if (v1[n] != v2[n]) return(FALSE)
        n <- n + 1;
    }
    return(TRUE)
}
combine.sets <- function(set1, set2){
    names1 <- names(set1);
    names2 <- names(set2);
    if (is.null(names1)) {
        stop("first set is NULL")
    } else if (is.null(names2)) {
        stop("second set is NULL")
    } 
    vars1 <- length(names1);
    vars2 <- length(names2);
    if (vars1 != vars2) {
        stop("set mismatch: num vars not equal")
    } else if (is.equal.vector(names2, names1)) {
        stop("sets mismatch: variable names not equal")
    } 
    new.set <- set1;
    for (var in names1) {
        new.set$var <- c(set1$var, set2$var);
    }
    return(new.set)
}
get.estimate <- function(model.ws, prev.set, year.index) {
    #debug point! print("in get.estimate");
    past.set.length <- length(prev.set$times);
    #prev.set <- c(prev.set, get.this.set(year.index-1));
    prev.set <- combine.sets(prev.set, get.this.set(year.index-1));
    if (length(prev.set$times)==past.set.length) {
      return(list(FALSE, prev.set))
    }
    #debug point! print("prev.set");
    wv <- gen.full.wv(model.wv = model.ws, num.years = year.index-1);
    #debug point! print(length(wv));
    prev.gv <- get.set.gridvals(prev.set);
    #debug point! print("prev.gv - grid values");
    #debug point! print(length(prev.gv$xg));
    #debug point! print(length(prev.gv$yg));
    #debug point! print(length(prev.set$longitude));
    #debug point! print(length(prev.set$latitude));
    #debug point! print(length(prev.set$pieces_km2))
    pred.bi <- interpBarnes(x = prev.set$longitude, y = prev.set$latitude, z = prev.set$pieces_km2, w = wv, xg = prev.gv$xg, yg = prev.gv$yg, iterations = 1);
    #debug point! print("pred.bi");
    return(list(pred.bi, prev.set))
}
get.actual <- function(year.index) {
    half.set <- subset(workingset.seamicro, workingset.seamicro$times>=range.years.ws$Start[year.index]);
    this.set <- subset(half.set, half.set$times<=range.years.ws$End[year.index]);
    if(length(this.set$times)<=0) {
        gv <- get.set.gridvals(workingset.seamicro);
        x <- length(gv$xg);
        y <- length(gv$yg);
        wg <- Matrix(data = rep(0, times = x*y), ncol = x, nrow = y);
        zg <- wg;
        zd <- rep(0, times = x);
        return(list("xg" = gv$xg, "yg" = gv$yg, "zg" = zg, "wg" = wg, "zd" = zd));
    }
    gv <- get.set.gridvals(this.set);
    this.bi <- interpBarnes(x = this.set$longitude, y = this.set$latitude, z = this.set$pieces_km2, xg = gv$xg, yg = gv$yg, iterations = 1);
    return(this.bi);
}
get.v.squared <- function(v) {
    v.rssquared <- 0;
    for (n in v) {
        v.rssquared <- v.rssquared + n^2;
    }
    return(v.rssquared)
}
get.r.squared <- function(y1, y) {
    shared.diff <- y - y1;
    sum.resid.squared <- get.v.squared(shared.diff);
    sum.act.squared <- get.v.squared(y);
    r.squared.shared <- 1 - sum.resid.squared/sum.act.squared;
    return(r.squared.shared)
}
get.m.from.xy <- function(m, x, y) {
    xlen <- length(x);
    ylen <- length(y);
    new.m <- matrix(nrow = xlen, ncol = ylen, dimnames = list(x, y));
    for (i in rownames(m)) {
        #goes through the row names of rows in m
        if(!is.null(only.in.v(i, x))) {
            #if i is a row name in x
            sus.col <- m[i,];
            for (j in colnames(m)) {
                #testing col names in m
                if (!is.null(only.in.v(j, y))){
                    #if this column is in y
                    new.m[i, j] <- m[i,j];
                }
                #if column isn't in y, then (i,j) isn't needed in new.m
            }
        }
        #if row isn't in x, then row i  isn't needed in new.m
    }
    if (is.na(new.m[xlen, ylen])) return(FALSE)
    return(new.m)
}
get.r.squared.total <- function(est.bi, actual.bi) {
    if (length(actual.bi$xg)==0) return(0)
    xover <- only.in.v(est.bi$xg, actual.bi$xg);
    #debug point! print(xover);
    if (xover[1] == FALSE) {
        act.bi.ungridded <- ungrid(actual.bi$xg, actual.bi$yg, actual.bi$zg);
        #debug point! print(length(act.bi.ungridded$grid));
        act.zv <- act.bi.ungridded$grid;
        act.resid.sq <- get.v.squared(act.zv);
        return(act.resid.sq)
    }
    front.x <- actual.bi$xg[actual.bi$xg<xover[1]];
    back.x <- actual.bi$xg[actual.bi$xg>xover[length(xover)]];
    shared.x <- xover;
    #now have all the shared x values and the outside values in x
    yover <- only.in.v(est.bi$yg, actual.bi$yg);
    if (yover[1] == FALSE) {
        act.bi.ungridded <- ungrid(actual.bi$xg, actual.bi$yg, actual.bi$zg);
        act.zv <- act.bi.ungridded$grid;
        act.resid.sq <- get.v.squared(act.zv);
        return(act.resid.sq)
    }
    top.y <- actual.bi$yg[actual.bi$yg<yover[1]];
    end.y <- actual.bi$yg[actual.bi$yg>yover[length(yover)]];
    shared.y <- yover;
    #now have all the shared y values and the outside values in y
    est.m <- get.m.from.xy(est.bi$zg, shared.x, shared.y);
    act.m <- get.m.from.xy(actual.bi$zg, shared.x, shared.y);
    front.m <- get.m.from.xy(actual.bi$zg, front.x, shared.y);
    back.m <- get.m.from.xy(actual.bi$zg, back.x, shared.y);
    top.m <- get.m.from.xy(actual.bi$zg, actual.bi$xg, top.y);
    end.m <- get.m.from.xy(actual.bi$zg, actual.bi$xg, end.y);
    #now have all the matrices we need
    r.sq.tot <- 0;
    front.mv <- as.vector(front.m);
    front.r.squared <- get.v.squared(front.mv);
    r.sq.tot <- r.sq.tot + front.r.squared;
    #have the r-squared of the FRONT matrix in total
    back.mv <- as.vector(back.m);
    back.r.squared <- get.v.squared(back.mv);
    r.sq.tot <- r.sq.tot + back.r.squared;
    #have the r-squared of the BACK matrix in total
    top.mv <- as.vector(top.m);
    top.r.squared <- get.v.squared(top.mv);
    r.sq.tot <- r.sq.tot + top.r.squared;
    #have the r-squared of the TOP matrix in total
    end.mv <- as.vector(end.m);
    end.r.squared <- get.v.squared(end.mv);
    r.sq.tot <- r.sq.tot + end.r.squared;
    #have the r-squared of the END matrix in total
    est.mv <- as.vector(est.m);
    act.mv <- as.vector(act.m);
    shared.r.squared <- get.r.squared(est.mv, act.mv);
    r.sq.tot <- r.sq.tot + shared.r.squared;
    #have the TOTAL r-squared
    return(r.sq.tot)
}
```

### MODEL 1: EQUAL WEIGHTS

```{r}
m1.rsq.v <- 0;
#because the first year has no estimate, it stays a zero
year.index <- 2
past.set <- get.this.set(1)
while (year.index <= length(range.years.ws$End)) {
  #debug point!
  #debug point! 
  print(year.index)
  #debug point! if(year.index>12) stop() year.index <- 12
  past.group <- get.estimate(model1.weights, past.set, year.index)
  if (past.group[1] == FALSE) {
    m1.rsq.v <- c(m1.rsq.v, 0)
    year.index <- year.index + 1
    next
  }
  past.bi <- past.group[1]
  past.set <- past.group[2]
  #debug point!
  #debug point! print("past.bi")
  past.bi <- set.bi.dimnames(past.bi)
  #debug point!
  #debug point! print("past.bi has names")
  now.bi <- get.actual(year.index)
  #debug point!
  #debug point! print("now.bi")
  now.bi <- set.bi.dimnames(now.bi)
  #debug point!
  #debug point! print("now.bi has names")
  now.rsq <- get.r.squared.total(past.bi, now.bi)
  #debug point!
  #debug point! 
  print(now.rsq)
  m1.rsq.v <- c(m1.rsq.v, now.rsq)
  year.index <- year.index + 1
}
print(m1.rsq.v)
```

## CONCLUSION

### POTENTIAL USES

## BIBLIOGRAPHY
